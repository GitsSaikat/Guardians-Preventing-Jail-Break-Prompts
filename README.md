# Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System

We have experimented and developed a defensive system for AI agents to protect themselves against adversarial prompt attacks and jailbreak attempts.

## Overview

This repository implements self-defending mechanisms for AI systems, enabling them to:
- Detect and prevent prompt injection attacks
- Maintain operational boundaries
- Filter malicious instructions
- Preserve their core directives
- Self-validate responses

## Core Components

- Automated prompt threat detection
- Self-validation mechanisms
- Response integrity checks
- Boundary enforcement
- Defensive prompt patterns

## Usage

The system actively monitors and defends against attempts to:
- Override core instructions
- Bypass safety measures
- Manipulate system behavior
- Inject malicious prompts

For implementation details, check the examples in `/experiments` directory.